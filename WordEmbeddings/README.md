Because word embeddings are very computationally expensive to train, most ML practitioners will load a pre-trained set of embeddings.
you'll use 50-dimensional GloVe vectors to represent words