{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because word embeddings are very computationally expensive to train, most ML practitioners will load a pre-trained set of embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, the word embeddings used will be a 50-dimensional GloVe vectors to represent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similarity between u and v.\n",
    "    cos(u, v) = u * v / (||u|| * ||v||)\n",
    "\n",
    "    Arguments:\n",
    "    u, v -- embeddings representation of words\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity -- the cosine angle between the 2 vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base case, u and v are the same\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "\n",
    "    dot = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "\n",
    "    # Avoid division by 0\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Function that finds a word for the task: a is to b as c is to _____.\n",
    "    e_b - e_a ~ e_w - e_c\n",
    "\n",
    "    Arguments:\n",
    "    word_a, word_b, word_c -- input words for the phrase\n",
    "    word_to_vec_map -- dictionary that maps a word to the corresponding embedded vectors.\n",
    "\n",
    "    Returns:\n",
    "    best_word -- best word to fit the analogy between a and b, for c\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the words into lowercase.\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "\n",
    "    # Get the embedded representation for each.\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "\n",
    "    # Get all the words to iter\n",
    "    words = word_to_vec_map.keys()\n",
    "\n",
    "    # Variables to find the most similar one to analogy\n",
    "    max_cosine_sim = -100\n",
    "    best_word = None\n",
    "\n",
    "    for word in words:\n",
    "        # Skip for the same word\n",
    "        if word == word_c:\n",
    "            continue\n",
    "    \n",
    "        # Get embedded version\n",
    "        e_word = word_to_vec_map[word]\n",
    "\n",
    "        # Check for similarity\n",
    "        sim = cosine_similarity(e_b - e_a, e_word - e_c)\n",
    "\n",
    "        if sim > max_cosine_sim:\n",
    "            max_cosine_sim = sim\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man is to woman, as boy is to: girl\n",
      "Man is to programmer, as woman is to: programmer\n"
     ]
    }
   ],
   "source": [
    "# Test analogy\n",
    "print(\"Man is to woman, as boy is to: \" + complete_analogy(\"Man\", \"woman\", \"boy\", word_to_vec_map))\n",
    "print(\"Man is to programmer, as woman is to: \" + complete_analogy(\"Man\", \"programmer\", \"woman\", word_to_vec_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
      " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
      "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
      "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
      "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
      " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
      " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
      "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
      " -0.04371     0.01258   ]\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the vector g that roughly encodes the concept of \"gender\".\n",
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the similarity cosine function to see which names are closer to our female gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "john -0.23163356145973732\n",
      "marie 0.31559793539607295\n",
      "sophie 0.3186878985941879\n",
      "ronaldo -0.31244796850329437\n",
      "priya 0.17632041839009405\n",
      "rahul -0.16915471039231728\n",
      "danielle 0.24393299216283904\n",
      "reza -0.07930429672199556\n",
      "katy 0.2831068659572616\n",
      "yasmin 0.23313857767928767\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# Girls and boys names.\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feminine names are closer to out \"female gender\" vector, which is expected.\n",
    "Let's print the same computation but now using jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "lipstick 0.27691916256382676\n",
      "guns -0.18884855678988982\n",
      "science -0.060829065409297015\n",
      "arts 0.008189312385880351\n",
      "literature 0.06472504433459933\n",
      "warrior -0.20920164641125286\n",
      "doctor 0.11895289410935046\n",
      "tree -0.0708939917547809\n",
      "receptionist 0.3307794175059374\n",
      "technology -0.13193732447554302\n",
      "fashion 0.035638946257727\n",
      "teacher 0.17920923431825672\n",
      "engineer -0.08039280494524072\n",
      "pilot 0.0010764498991917212\n",
      "computer -0.10330358873850498\n",
      "singer 0.18500518136496294\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words are closer to female gender then male and vice-versa. \n",
    "It is astonishing how these results reflect certain unhealthy gender stereotypes.\n",
    "For example, we see “computer” is negative and is closer in value to male first names, while “literature” is positive and is closer to female first names. Ouch!\n",
    "\n",
    "You'll see below how to reduce the bias of these vectors, using an algorithm due to `Paperworks/Debiasing-word-embeddings.pdf`.\n",
    "Note that some word pairs such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather\" should remain gender-specific,\n",
    "while other words such as \"receptionist\" or \"technology\" should be neutralized, i.e. not be gender-related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:00<00:00, 453093.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# The paper assumes all word vectors to have L2 norm as 1 and hence the need for this calculation\n",
    "from tqdm import tqdm\n",
    "word_to_vec_map_unit_vectors = {\n",
    "    word: embedding / np.linalg.norm(embedding)\n",
    "    for word, embedding in tqdm(word_to_vec_map.items())\n",
    "}\n",
    "g_unit = word_to_vec_map_unit_vectors['woman'] - word_to_vec_map_unit_vectors['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduse bias by fixing a bias and a non-bias axis and project the non-gender words onto non-bias axis.\n",
    "e_bias_component = (e * g / (||g|| ^ 2)) * g -- projection onto bias axis.\n",
    "e_dibiased = e - e_bias_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\".\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula given above.\n",
    "    e_biascomponent = np.dot(e, g) / (np.linalg.norm(g) ** 2) * g\n",
    " \n",
    "    # Neutralize e by subtracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection.\n",
    "    e_debiased = e - word_to_vec_map[word]\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the debiasing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.3307794175059374\n",
      "cosine similarity between receptionist and g_unit, after neutralizing:  0\n"
     ]
    }
   ],
   "source": [
    "word = \"receptionist\"\n",
    "print(\"cosine similarity between \" + word + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[word], g))\n",
    "\n",
    "e_debiased = neutralize(word, g_unit, word_to_vec_map_unit_vectors)\n",
    "print(\"cosine similarity between \" + word + \" and g_unit, after neutralizing: \", cosine_similarity(e_debiased, g_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how debiasing can also be applied to word pairs such as \"actress\" and \"actor\".\n",
    "Equalization is applied to pairs of words that you might want to have differ only through the gender property.\n",
    "As a concrete example, suppose that \"actress\" is closer to \"babysit\" than \"actor\".\n",
    "By applying neutralization to \"babysit,\" you can reduce the gender stereotype associated with babysitting.\n",
    "But this still does not guarantee that \"actor\" and \"actress\" are equidistant from \"babysit.\" The equalization algorithm takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described in the figure above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\".\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Compute the mean of e_w1 and e_w2.\n",
    "    mu = 1 / 2 * (e_w1 + e_w2)\n",
    "\n",
    "    # Compute the projections of mu over the bias axis and the orthogonal axis.\n",
    "    mu_B = np.dot(mu, bias_axis) / (np.linalg.norm(bias_axis) ** 2) * bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Compute e_w1B and e_w2B.\n",
    "    e_w1B = np.dot(e_w1, bias_axis) / (np.linalg.norm(bias_axis) ** 2) * bias_axis\n",
    "    e_w2B = np.dot(e_w2, bias_axis) / (np.linalg.norm(bias_axis) ** 2) * bias_axis\n",
    "        \n",
    "    # Adjust the Bias part of e_w1B and e_w2B.\n",
    "    corrected_e_w1B = np.sqrt(1 - np.linalg.norm(mu) ** 2) * (e_w1B - mu_B) / np.linalg.norm(e_w1B - mu_B)\n",
    "    corrected_e_w2B = np.sqrt(1 - np.linalg.norm(mu) ** 2) * (e_w2B - mu_B) / np.linalg.norm(e_w2B - mu_B)\n",
    "\n",
    "    # Debias by equalizing e1 and e2 to the sum of their corrected projections\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "                                                                \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.11711095765336835\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.35666618846270376\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.2387113614288377\n",
      "cosine_similarity(e2, gender) =  0.2387113614288377\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g_unit, word_to_vec_map_unit_vectors)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g_unit))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g_unit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
